{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coqui Testing\n",
    "\n",
    "This is a notebook just hacking around with Coqui TTS.\n",
    "Intention is to try the various models and see what's what.\n",
    "\n",
    "I found that there wasn't a whole lot of examples out there for using the various models. Maybe this will be helpful to you. Maybe not.\n",
    "\n",
    "Note that for my own purposes, I just searched for a Jennifer Garner video on youtube and used one that I found as a source voice. In case there are leftover references to Jennifer Garner in this notebook.\n",
    "\n",
    "All of these examples were run on an Nvidia RTX 4070ti GPU. \n",
    "\n",
    "| Model | Processing Time (s) | VRAM Usage | Quality (H/M/L rated by me) | Voice Cloning |\n",
    "|-------|---------------------|------------|-----------------------------|---------------|\n",
    "| XttsV2 | 12.4s               |  2.3 GB     |   H                   | Y |\n",
    "| Speedy Speech |  2s  | 0.7 GB | L | N | \n",
    "| Your TTS | 1.2s   | 0.7 GB | M | Y |\n",
    "| Bark | 105s | 4.4 GB | L | Y?? |\n",
    "\n",
    "Before using the notebook, create a venv and pip install tts. Also note that, as of this notebook date, coqui tts install doesn't work with python 3.12. So when creating your venv, probably best to use 3.11 or 3.10. Easy enough (ex. python3.11 -m venv .\\venv)\n",
    "\n",
    "Because the default tts from pip won't include any GPU accelerated pytorch on Windows, you will likely need to pip install the cu version of Pytorch. Typically I just Google pytorch cuda install and the top hit is the page that gives you the proper pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script will be used throughout the notebook. It was a stream of thought.\n",
    "\n",
    "script = ('I had this dream... So strange... Like, I was this little turtle. Crawling... so slowly.'\n",
    "          'And there was this giant tiger behind me. Just staring at me...'\n",
    "          'It seemed just so, sad.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XTTSv2\n",
    "\n",
    "The following is just hacking around with XTTSv2 with Coqui. It's a really really amazing TTS model that allows voice cloning and it does it so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is ripped right out of the Coqui docs.\n",
    "\n",
    "On a 4070ti, xttsv2 takes about 28 seconds to load the model and then generate output with the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\dev\\tts\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n",
      " > Processing time: 13.626079559326172\n",
      " > Real-time factor: 0.6809951366345015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_xttsv2_1.wav'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "# Run TTS\n",
    "# ‚ùó Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n",
    "# Text to speech to a file\n",
    "tts.tts_to_file(text=script, \n",
    "                speaker_wav=\"samples/jennifergarner.wav\", \n",
    "                language=\"en\", \n",
    "                file_path=\"output/output_xttsv2_1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_xttsv2_1.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded, voice generation takes a fraction of the time.\n",
    "On my 4070ti, the following code takes 12.5s\n",
    "Interestingly, takes longer with the xtts streaming server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n",
      " > Processing time: 12.358268737792969\n",
      " > Real-time factor: 0.6098485016031415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_xttsv2_2.wav'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(text=script, \n",
    "                speaker_wav=\"samples/jennifergarner.wav\", \n",
    "                language=\"en\", \n",
    "                file_path=\"output/output_xttsv2_2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_xttsv2_2.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all the Coqui models anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code dumps the models to a json file for viewing. Formats really well in vscode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "models = TTS().list_models()\n",
    "\n",
    "with open('models.json', 'w') as f:\n",
    "    json.dump(models.models_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try other models..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedy Speech\n",
    "\n",
    "Trying out speedy speech. Although much faster than xttsv2, it doesn't do voice cloning, as far as I can tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device, cuda\n",
      " > tts_models/en/ljspeech/speedy-speech is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: speedy_speech\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n",
      "so ä st…πe…™ndÕ° í...\n",
      " [!] Character 'Õ°' not found in the vocabulary. Discarding it.\n",
      " > Processing time: 4.207754373550415\n",
      " > Real-time factor: 0.2870840881256085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_speedy_speech.wav'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('using device, ' + device)\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(\"tts_models/en/ljspeech/speedy-speech\").to(device)\n",
    "\n",
    "# Run TTS\n",
    "# Text to speech to a file\n",
    "tts.tts_to_file(text=script, \n",
    "                speaker_wav=\"samples/jennifergarner.wav\", \n",
    "                file_path=\"output/output_speedy_speech.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_speedy_speech.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n",
      " > Processing time: 0.40735602378845215\n",
      " > Real-time factor: 0.02779283728320514\n"
     ]
    }
   ],
   "source": [
    "tts.tts_with_vc_to_file(\n",
    "    script,\n",
    "    speaker_wav=\"samples/jennifergarner.wav\",\n",
    "    file_path=\"output/output_speedyspeech2.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_speedyspeech2.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.3594348430633545\n",
      " > Real-time factor: 0.024523300316683275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_speedy_speech3.wav'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(text=script, \n",
    "                file_path=\"output/output_speedy_speech3.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your TTS\n",
    "\n",
    "Your TTS does voice cloning but the quality is really not that great. You can kind of hear the Jennifer Garner in these outputs, but just barely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n",
      " > External Speaker Encoder Loaded !!\n",
      " > initialization of language-embedding layers.\n",
      " > Model fully restored. \n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:16000\n",
      " | > resample:False\n",
      " | > num_mels:64\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:512\n",
      " | > power:1.5\n",
      " | > preemphasis:0.97\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:False\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:True\n",
      " | > db_level:-27.0\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:160\n",
      " | > win_length:400\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n",
      " > Processing time: 1.6458244323730469\n",
      " > Real-time factor: 0.10130020510697649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_yourtts_en.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(script, \n",
    "                speaker_wav=\"samples/example_reference.mp3\", \n",
    "                language=\"en\", \n",
    "                file_path=\"output/output_yourtts_en.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_yourtts_en.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Going to try just sending the english script through to the different languages. Also note that the docs aren't quite correct for specifying the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 1.7092227935791016\n",
      " > Real-time factor: 0.1386568340698549\n",
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n",
      " > Processing time: 1.4384911060333252\n",
      " > Real-time factor: 0.08454252753648693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_yourtts_pt-br.wav'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(script, speaker_wav=\"samples/jennifergarner.wav\", language=\"fr-fr\", file_path=\"output/output_yourtts_fr-fr.wav\")\n",
    "tts.tts_to_file(script, speaker_wav=\"samples/jennifergarner.wav\", language=\"pt-br\", file_path=\"output/output_yourtts_pt-br.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_yourtts_fr-fr.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_yourtts_pt-br.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bark\n",
    "\n",
    "Let's now try Bark :)\n",
    "\n",
    "(when I first ran this, Coqui tried to download the model files, but a couple of the .pth files got corrupted. May have to manually download the larger files from here : https://huggingface.co/suno/bark/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/bark is already downloaded.\n",
      " > Using model: bark\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/bark\", progress_bar=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I had this dream...', 'So strange...', 'Like, I was this little turtle.', 'Crawling... so slowly.', 'And there was this giant tiger behind me.', 'Just staring at me..', '.It seemed just so, sad.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\dev\\tts\\venv\\Lib\\site-packages\\TTS\\tts\\layers\\bark\\model.py:87: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout, is_causal=is_causal)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 14.23it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:17<00:00,  1.24it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 28.11it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:09<00:00,  1.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 77.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 50.55it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:05<00:00,  1.25it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 63.56it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:04<00:00,  1.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 48.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.33it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.00it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 104.59143352508545\n",
      " > Real-time factor: 2.7478804561387546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/output_bar_1.wav'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(text=script, \n",
    "                speaker_wav=\"samples/jennifergarner.wav\", \n",
    "                file_path=\"output/output_bar_1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"./output/output_bar_1.wav\" type=\"audio/wav\">\n",
    "  Your browser does not support the audio element.\n",
    "</audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XTTS Manual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\dev\\tts\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Computing speaker latents...\n",
      "Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\dev\\tts\\venv\\Lib\\site-packages\\TTS\\tts\\layers\\xtts\\stream_generator.py:138: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to first chunck: 0.9614393711090088\n",
      "Received chunk 0 of audio length 21248\n",
      "Received chunk 1 of audio length 22272\n",
      "Received chunk 2 of audio length 22272\n",
      "Received chunk 3 of audio length 22272\n",
      "Received chunk 4 of audio length 22272\n",
      "Received chunk 5 of audio length 22272\n",
      "Received chunk 6 of audio length 22272\n",
      "Received chunk 7 of audio length 1024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "print(\"Loading model...\")\n",
    "config = XttsConfig()\n",
    "config.load_json(\"C:\\\\Users\\\\jamie\\\\AppData\\\\Local\\\\tts\\\\tts_models--multilingual--multi-dataset--xtts_v2\\\\config.json\")\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_dir=\"C:\\\\Users\\\\jamie\\\\AppData\\\\Local\\\\tts\\\\tts_models--multilingual--multi-dataset--xtts_v2\\\\\", use_deepspeed=False)\n",
    "model.cuda()\n",
    "\n",
    "print(\"Computing speaker latents...\")\n",
    "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=[\".\\\\samples\\\\example_reference.mp3\"])\n",
    "\n",
    "print(\"Inference...\")\n",
    "t0 = time.time()\n",
    "chunks = model.inference_stream(\n",
    "    \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n",
    "    \"en\",\n",
    "    gpt_cond_latent,\n",
    "    speaker_embedding\n",
    ")\n",
    "\n",
    "wav_chuncks = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i == 0:\n",
    "        print(f\"Time to first chunck: {time.time() - t0}\")\n",
    "    print(f\"Received chunk {i} of audio length {chunk.shape[-1]}\")\n",
    "    wav_chuncks.append(chunk)\n",
    "wav = torch.cat(wav_chuncks, dim=0)\n",
    "torchaudio.save(\"xtts_streaming.wav\", wav.squeeze().unsqueeze(0).cpu(), 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference...\n",
      "Time to first chunck: 0.793259859085083\n",
      "Received chunk 0 of audio length 21248\n",
      "Received chunk 1 of audio length 22272\n",
      "Received chunk 2 of audio length 22272\n",
      "Received chunk 3 of audio length 22272\n",
      "Received chunk 4 of audio length 22272\n",
      "Received chunk 5 of audio length 22272\n",
      "Received chunk 6 of audio length 22272\n",
      "Received chunk 7 of audio length 22272\n",
      "Received chunk 8 of audio length 22272\n",
      "Received chunk 9 of audio length 22272\n",
      "Received chunk 10 of audio length 22272\n",
      "Received chunk 11 of audio length 22272\n",
      "Received chunk 12 of audio length 22272\n",
      "Received chunk 13 of audio length 22528\n",
      "Received chunk 14 of audio length 22272\n",
      "Received chunk 15 of audio length 22272\n",
      "Received chunk 16 of audio length 22272\n",
      "Received chunk 17 of audio length 22272\n",
      "Received chunk 18 of audio length 4352\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "print(\"Inference...\")\n",
    "t0 = time.time()\n",
    "chunks = model.inference_stream(\n",
    "    script,\n",
    "    \"en\",\n",
    "    gpt_cond_latent,\n",
    "    speaker_embedding\n",
    ")\n",
    "\n",
    "wav_chuncks = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i == 0:\n",
    "        print(f\"Time to first chunck: {time.time() - t0}\")\n",
    "    print(f\"Received chunk {i} of audio length {chunk.shape[-1]}\")\n",
    "    wav_chuncks.append(chunk)\n",
    "wav = torch.cat(wav_chuncks, dim=0)\n",
    "\n",
    "torchaudio.save(\"xtts_streaming.wav\", wav.squeeze().unsqueeze(0).cpu(), 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
